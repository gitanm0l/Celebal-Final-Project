{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881eab33",
   "metadata": {},
   "source": [
    "# Data Ingestion Notebook\n",
    "This notebook loads transaction and product data from ADLS Gen2, joins them, and performs basic analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"DataIngestion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ADLS paths (change these paths to your actual container and folder)\n",
    "transaction_path = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/transactions.csv\"\n",
    "product_path = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/products.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV into DataFrames\n",
    "df_transactions = spark.read.option(\"header\", True).csv(transaction_path)\n",
    "df_products = spark.read.option(\"header\", True).csv(product_path)\n",
    "\n",
    "# Show sample data\n",
    "df_transactions.show(5)\n",
    "df_products.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2902ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join both DataFrames on product ID\n",
    "df_joined = df_transactions.join(df_products, df_transactions.product_id == df_products.product_id, \"inner\")\n",
    "df_joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc9fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average order value for each customer\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "avg_order_value = df_joined.groupBy(\"customer_id\").agg(avg(col(\"price\")).alias(\"avg_order_value\"))\n",
    "avg_order_value.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0413a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify popular products\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "popular_products = df_joined.groupBy(\"product_id\", \"product_name\").agg(count(\"*\").alias(\"orders\")).orderBy(\"orders\", ascending=False)\n",
    "popular_products.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "df_joined.select([col(c).isNull().alias(c + \"_is_null\") for c in df_joined.columns]).show(5)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
